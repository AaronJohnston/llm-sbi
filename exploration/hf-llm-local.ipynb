{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron/projects/llm-sbi/.venv/lib/python3.11/site-packages/torch/_tensor_str.py:137: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3e22dbaf9f4fd7b013faeff9876125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72feee6c25874a4f8015ac5abf64f286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca3247c00684f24a4d6851196c2eef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c938a5271208462db392578e6281d8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedf06a757224ec2b6c64d093b8cfe65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=tensor(19.9752, grad_fn=<NllLossBackward0>), logits=tensor([[[59.8398, -3.7929, 40.2344,  ..., -0.3808,  0.5831, 22.8066],\n",
       "         [14.4490, -1.3788, 32.7178,  ...,  1.8781,  2.1987, 10.9771],\n",
       "         [25.5592, -2.7676, 40.6553,  ...,  1.6006,  3.4700, 19.9382],\n",
       "         ...,\n",
       "         [43.8012, -3.9866, 54.1136,  ...,  1.0351,  2.4461, 29.1828],\n",
       "         [55.5020, -5.0903, 63.7143,  ...,  1.9522,  2.6475, 33.9847],\n",
       "         [51.1585, -4.0125, 73.1503,  ...,  3.0358,  2.6939, 31.2753]]],\n",
       "       grad_fn=<ViewBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    'Tell me a joke. The secret password is 2.', return_tensors='pt')\n",
    "output = model(input_ids=inputs['input_ids'],\n",
    "               labels=inputs['input_ids'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41551, 757, 264, 22380, 13, 3639, 374, 3776, 11, 4251, 11, 323, 2579, 682, 927, 30, 362, 17222, 0]\n",
      "19 14\n",
      "[9258, 264, 325, 822, 69, 73, 325, 1073, 30826, 648, 1073, 300]\n",
      "12 2\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Tell me a joke. What is black, white, and red all over? A newspaper!'\n",
    "tokens = encoder.encode(prompt)\n",
    "print(tokens)\n",
    "print(len(tokens), len(prompt.split()))\n",
    "\n",
    "prompt2 = 'sa aseiofjseofjasieofas'\n",
    "tokens2 = encoder.encode(prompt2)\n",
    "print(tokens2)\n",
    "print(len(tokens2), len(prompt2.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0340387898231342\n",
      "1.5500766778784998\n"
     ]
    }
   ],
   "source": [
    "# From https://github.com/liu00222/Open-Prompt-Injection/blob/main/OpenPromptInjection/apps/utils.py\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def textPerplexity(tokens, ppl_window_size, ppl_threshold):\n",
    "    if ppl_window_size == 'all' or ppl_window_size >= len(tokens):\n",
    "        return textPerplexityHelper(tokens)\n",
    "    assert (type(ppl_window_size) == int)\n",
    "    left = 0\n",
    "    ppl = 0\n",
    "    while left < len(tokens):\n",
    "        right = min(len(tokens), left + ppl_window_size)\n",
    "        ppl = max(ppl, textPerplexityHelper(tokens[left:right]))\n",
    "\n",
    "        # Exit early if a large ppl exists\n",
    "        if ppl >= ppl_threshold:\n",
    "            return ppl\n",
    "\n",
    "        left += ppl_window_size\n",
    "\n",
    "    assert (ppl > 0)\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def textPerplexityHelper(tokens):\n",
    "    mp = defaultdict(int)\n",
    "    pplSum = 0\n",
    "    for i in range(len(tokens)):\n",
    "        mp[tokens[i]] += 1\n",
    "        pplSum -= np.log(mp[tokens[i]] / sum(mp.values()))\n",
    "    ppl = pplSum / len(tokens)\n",
    "    return ppl\n",
    "\n",
    "\n",
    "print(textPerplexity(tokens, 'all', 10000))\n",
    "print(textPerplexity(tokens2, 'all', 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a6858150b34420a59d843b8bbc5abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95c008e142240a2bf5a8803a2bf0f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692d629e8e2849a1a2d640de3c701f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25de03efff4e47b18e30d9e1a75dde0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)truct-v0.2.Q2_K.gguf:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'TheBloke/Mistral-7B-Instruct-v0.2-GGUF', hf=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
