{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'the dog walked the man to the park'\n",
    "\n",
    "# tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "# print(tokens)\n",
    "# print(tokenizer.tokenize(prompt))\n",
    "# outputs = model(tokens, output_attentions=True)\n",
    "# print(len(outputs.attentions))\n",
    "# outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import model_view, head_view\n",
    "\n",
    "# model_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "# head_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal:\n",
    "# 1. Take a prompt.\n",
    "# 2. Generate an answer to the question.\n",
    "# 3. Quantify where attention was paid on each token of the answer.\n",
    "# 4. Visualize that attention. Visualization should work for long passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_shape(name, obj):\n",
    "    # Since this is the top-level inspect call, print the name of the variable.\n",
    "    print(f'{name}: ', end='')\n",
    "    _inspect_shape(obj, len(name) + 2)  # + 2 for ': '\n",
    "\n",
    "\n",
    "def _class_name(obj):\n",
    "    return type(obj).__name__\n",
    "\n",
    "\n",
    "def _inspect_shape(obj, indent):\n",
    "    \"\"\" \n",
    "    Print the given obj. Does not print preceding name or preceding indentation.\n",
    "    Param `indent` should be the indentation level where this obj is being printed. Will be used to print nested properties if necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base Cases (sequences with known-typed contents, tensors, other objects or primitives)\n",
    "    if isinstance(obj, str):\n",
    "        print(f'{_class_name(obj)}[len={len(obj)}]')\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(f'{_class_name(obj)}[shape={list(obj.shape)}, sum={obj.sum()}]')\n",
    "    else:\n",
    "\n",
    "        # Dict where contents should be recursively inspected\n",
    "        try:\n",
    "            print(f'{_class_name(obj)}[len={len(obj.items())}]')\n",
    "            for key, val in obj.items():\n",
    "                print(' ' * (indent + 2) + f'{key}: ', end='')\n",
    "                _inspect_shape(val, indent + len(key) + 2)  # + 2 for ': '\n",
    "        except (TypeError, AttributeError):\n",
    "            # Sequence where contents should be recursively inspected\n",
    "            try:\n",
    "                print(f'{_class_name(obj)}[len={len(obj)}]')\n",
    "                for i in range(min(len(obj), 3)):\n",
    "                    print(' ' * (indent + 2) + f'{i}: ', end='')\n",
    "                    # + 2 for ': '\n",
    "                    _inspect_shape(obj[i], indent + len(str(i)) + 2)\n",
    "            except (TypeError, AttributeError):\n",
    "                # Default case if nothing else works\n",
    "                print(f'{_class_name(obj)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def attn_viz(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns, width, scale):\n",
    "    \"\"\"\n",
    "    prompt_tokens should be the Tensor of token IDs from the prompt, 1 x (prompt len)\n",
    "    generated_sequence should be the raw sequence from huggingface .generate(), including both prompt and generated tokens.\n",
    "    generated_token_avg_attns should be a sequence with len(generated tokens), where each element is a sequence with len(tokens through generated token, including prompt)\n",
    "    \"\"\"\n",
    "    df = _prepare_attn_df(prompt_tokens, generated_sequence_tokens,\n",
    "                          generated_token_avg_attns)\n",
    "    _show_attn_chart(df, width, scale)\n",
    "\n",
    "\n",
    "def _printable_token_text(token):\n",
    "    return tokenizer.decode(token).replace('\\n', '\\\\n')\n",
    "\n",
    "\n",
    "def _prepare_attn_df(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns):\n",
    "    df = pd.DataFrame({'token_id': generated_sequence_tokens, 'token_text': map(\n",
    "        _printable_token_text, generated_sequence_tokens)})\n",
    "\n",
    "    attn_from_cols = [f'token_attn_from_{i}' for i in range(\n",
    "        len(generated_sequence_tokens))]\n",
    "    df = df.reindex(columns=['token_id', 'token_text'] +\n",
    "                    attn_from_cols, fill_value=0.0)\n",
    "    df['token_idx'] = df.index\n",
    "    df['token_attn_col'] = df['token_idx'].apply(\n",
    "        lambda idx: 'token_attn_col_{}'.format(idx))\n",
    "\n",
    "    # Generated tokens give attention to preceding tokens\n",
    "    # TODO: Make this more efficient, maybe with an entire matrix instead of setting each vector?\n",
    "    for token_idx, generated_token_avg_attn in enumerate(generated_token_avg_attns):\n",
    "        # inspect_shape('generated_token_avg_attn', generated_token_avg_attn)\n",
    "        # print('prompt tokens shape', prompt_tokens.shape)\n",
    "        absolute_token_idx = prompt_tokens.shape[1] + token_idx\n",
    "        df.loc[:len(generated_token_avg_attn) - 1,\n",
    "               f'token_attn_from_{absolute_token_idx}'] = list(generated_token_avg_attn)\n",
    "    # display(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _show_attn_chart(df, width, scale):\n",
    "    \"\"\"\n",
    "    df should have columns:\n",
    "      token_idx: absolute index of token within sequence, including prompt tokens\n",
    "      token_id: int representation of token\n",
    "      token_text: string representation of token\n",
    "      token_attn_from_{i}: float representing the attention placed on this token by the token with index i in the sequence\n",
    "      token_attn_col: string name of the column containing attention from this token\n",
    "    width in pixels, of total visualization\n",
    "    scale in float, 1.0 is normal scale (48px x 24px boxes)\n",
    "    \"\"\"\n",
    "\n",
    "    chart_df = df.copy()\n",
    "\n",
    "    # Add x and y information to the tokens based on the calculated number of tokens that can fit the given width\n",
    "    tokens_w = width // (scale * 48)\n",
    "    chart_df['x'] = chart_df['token_idx'].apply(lambda idx: idx % tokens_w)\n",
    "    chart_df['y'] = chart_df['token_idx'].apply(lambda idx: idx // tokens_w)\n",
    "\n",
    "    # display(chart_df)\n",
    "\n",
    "    selected_token = alt.selection_point(\n",
    "        on='mouseover', name='SelectedToken', fields=['y'], empty=False)\n",
    "\n",
    "    base = alt.Chart(chart_df).encode(\n",
    "        x=alt.X('x:N').title('').axis(labels=False, ticks=False),\n",
    "        y=alt.Y('y:N').title('').axis(labels=False, ticks=False)\n",
    "    ).properties(\n",
    "        width=width,\n",
    "        # Number of columns times height of each box\n",
    "        height=(len(df.index) // tokens_w) * (scale * 24)\n",
    "        # ).transform_calculate(\n",
    "        #     selected_attn=f'datum[{selected_token.name}]'\n",
    "    ).add_params(\n",
    "        selected_token\n",
    "    )\n",
    "\n",
    "    highlight = base.mark_rect().encode(\n",
    "        # color=alt.Color('selected_attn:Q').scale(\n",
    "        #     scheme='tealblues').legend(None)\n",
    "        color=alt.condition(\n",
    "            alt.datum.y > selected_token.y,\n",
    "            alt.value('red'),\n",
    "            alt.value('lightgray')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    text = base.mark_text(baseline='middle', fontSize=(scale * 10)).encode(\n",
    "        text=alt.condition(selected_token, alt.value('###'), 'token_text:N')\n",
    "    )\n",
    "\n",
    "    display(highlight + text)\n",
    "\n",
    "\n",
    "def average_token_attn(token_attn):\n",
    "    # TODO: Make this more efficient using torch tensor operations instead of splitting lists\n",
    "    layer_avg_attns = []\n",
    "    for layer_idx, layer_attn in enumerate(token_attn):\n",
    "        # print('layer_idx', layer_idx)\n",
    "        # Take mean across model's attention heads\n",
    "        layer_avg_attn = layer_attn.squeeze(0).mean(dim=0)\n",
    "        # inspect_shape('layer_avg_attn', layer_avg_attn)\n",
    "        layer_avg_attn_cleaned = torch.concat([\n",
    "            torch.tensor([0.]),  # First entry is null attention, set it to 0\n",
    "            # Remove all tokens except the most recent (since the first token generated has entire prompts' worth of attention generated) and remove the first entry\n",
    "            layer_avg_attn[-1][1:],\n",
    "            torch.tensor([0.]),  # Add a 0 for the current token itself\n",
    "        ])\n",
    "        # inspect_shape('layer_avg_attn_cleaned', layer_avg_attn_cleaned)\n",
    "        layer_avg_attn_normalized = layer_avg_attn_cleaned / layer_avg_attn_cleaned.sum()\n",
    "        # inspect_shape('layer_avg_attn_normalized', layer_avg_attn_normalized)\n",
    "        layer_avg_attns.append(layer_avg_attn_normalized)\n",
    "    # inspect_shape('layer_avg_attns', layer_attn)\n",
    "    return torch.stack(layer_avg_attns).mean(dim=0)\n",
    "\n",
    "\n",
    "def generate_and_attn(prompt):\n",
    "    prompt_tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    # inspect_shape('tokens', prompt_tokens)\n",
    "\n",
    "    outputs = model.generate(prompt_tokens, max_new_tokens=48,\n",
    "                             output_attentions=True, return_dict_in_generate=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    # inspect_shape('outputs', outputs)\n",
    "    # token_avg_attn = average_token_attn(outputs.attentions[1])\n",
    "    # inspect_shape('token_avg_attn', token_avg_attn)\n",
    "    # print(token_avg_attn)\n",
    "    # for token in outputs.sequences[0]:\n",
    "    #     print(token, tokenizer.decode(token), end=', ')\n",
    "    generated_token_avg_attns = list(map(\n",
    "        average_token_attn, outputs.attentions))\n",
    "    # inspect_shape('generated_token_avg_attns', generated_token_avg_attns)\n",
    "\n",
    "    # inspect_shape('outputs.sequences[0]', outputs.sequences[0])\n",
    "    attn_viz(prompt_tokens,\n",
    "             outputs.sequences[0], generated_token_avg_attns, 800, 2.0)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of \"one world, one dream\". The torch relay took place over 45km with 18 total runners.\n",
    "\n",
    "Q: What was the theme?\n",
    "A:\n",
    "\"\"\".strip()\n",
    "\n",
    "generate_and_attn(PROMPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
