{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'the dog walked the man to the park'\n",
    "\n",
    "# tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "# print(tokens)\n",
    "# print(tokenizer.tokenize(prompt))\n",
    "# outputs = model(tokens, output_attentions=True)\n",
    "# print(len(outputs.attentions))\n",
    "# outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import model_view, head_view\n",
    "\n",
    "# model_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "# head_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal:\n",
    "# 1. Take a prompt.\n",
    "# 2. Generate an answer to the question.\n",
    "# 3. Quantify where attention was paid on each token of the answer.\n",
    "# 4. Visualize that attention. Visualization should work for long passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_shape(name, obj):\n",
    "    # Since this is the top-level inspect call, print the name of the variable.\n",
    "    print(f'{name}: ', end='')\n",
    "    _inspect_shape(obj, len(name) + 2)  # + 2 for ': '\n",
    "\n",
    "\n",
    "def _class_name(obj):\n",
    "    return type(obj).__name__\n",
    "\n",
    "\n",
    "def _inspect_shape(obj, indent):\n",
    "    \"\"\" \n",
    "    Print the given obj. Does not print preceding name or preceding indentation.\n",
    "    Param `indent` should be the indentation level where this obj is being printed. Will be used to print nested properties if necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base Cases (sequences with known-typed contents, tensors, other objects or primitives)\n",
    "    if isinstance(obj, str):\n",
    "        print(f'{_class_name(obj)}[len={len(obj)}]')\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(f'{_class_name(obj)}[shape={list(obj.shape)}, sum={obj.sum()}]')\n",
    "    else:\n",
    "\n",
    "        # Dict where contents should be recursively inspected\n",
    "        try:\n",
    "            print(f'{_class_name(obj)}[len={len(obj.items())}]')\n",
    "            for key, val in obj.items():\n",
    "                print(' ' * (indent + 2) + f'{key}: ', end='')\n",
    "                _inspect_shape(val, indent + len(key) + 2)  # + 2 for ': '\n",
    "        except (TypeError, AttributeError):\n",
    "            # Sequence where contents should be recursively inspected\n",
    "            try:\n",
    "                print(f'{_class_name(obj)}[len={len(obj)}]')\n",
    "                for i in range(min(len(obj), 3)):\n",
    "                    print(' ' * (indent + 2) + f'{i}: ', end='')\n",
    "                    # + 2 for ': '\n",
    "                    _inspect_shape(obj[i], indent + len(str(i)) + 2)\n",
    "            except (TypeError, AttributeError):\n",
    "                # Default case if nothing else works\n",
    "                print(f'{_class_name(obj)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_token_avg_attns: list[len=48]\n",
      "                             0: Tensor[shape=[61], sum=1.0]\n",
      "                             1: Tensor[shape=[62], sum=1.0]\n",
      "                             2: Tensor[shape=[63], sum=1.0]\n",
      "generated_sequence_tokens: Tensor[shape=[108], sum=454166]\n",
      "list(generated_sequence_tokens): list[len=108]\n",
      "                                   0: Tensor[shape=[], sum=464]\n",
      "                                   1: Tensor[shape=[], sum=3648]\n",
      "                                   2: Tensor[shape=[], sum=10216]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_idx</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_text</th>\n",
       "      <th>attn_from_idx</th>\n",
       "      <th>attn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>464</td>\n",
       "      <td>The</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>464</td>\n",
       "      <td>The</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>464</td>\n",
       "      <td>The</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>464</td>\n",
       "      <td>The</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>464</td>\n",
       "      <td>The</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11659</th>\n",
       "      <td>107</td>\n",
       "      <td>625</td>\n",
       "      <td>over</td>\n",
       "      <td>103</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11660</th>\n",
       "      <td>107</td>\n",
       "      <td>625</td>\n",
       "      <td>over</td>\n",
       "      <td>104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11661</th>\n",
       "      <td>107</td>\n",
       "      <td>625</td>\n",
       "      <td>over</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11662</th>\n",
       "      <td>107</td>\n",
       "      <td>625</td>\n",
       "      <td>over</td>\n",
       "      <td>106</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11663</th>\n",
       "      <td>107</td>\n",
       "      <td>625</td>\n",
       "      <td>over</td>\n",
       "      <td>107</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11664 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       token_idx  token_id token_text  attn_from_idx  attn\n",
       "0              0       464        The              0   0.0\n",
       "1              0       464        The              1   0.0\n",
       "2              0       464        The              2   0.0\n",
       "3              0       464        The              3   0.0\n",
       "4              0       464        The              4   0.0\n",
       "...          ...       ...        ...            ...   ...\n",
       "11659        107       625       over            103   0.0\n",
       "11660        107       625       over            104   0.0\n",
       "11661        107       625       over            105   0.0\n",
       "11662        107       625       over            106   0.0\n",
       "11663        107       625       over            107   0.0\n",
       "\n",
       "[11664 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expression compilation error: No signal named SelectedToken in evaluation scope\n    Context[0]: Failed to get node value\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/llm-sbi/.venv/lib/python3.11/site-packages/IPython/core/formatters.py:974\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    971\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/llm-sbi/.venv/lib/python3.11/site-packages/altair/vegalite/v5/api.py:2564\u001b[0m, in \u001b[0;36mTopLevelMixin._repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrenderers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/llm-sbi/.venv/lib/python3.11/site-packages/altair/utils/display.py:227\u001b[0m, in \u001b[0;36mHTMLRenderer.__call__\u001b[0;34m(self, spec, **metadata)\u001b[0m\n\u001b[1;32m    224\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(metadata)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# To get proper return value type, would need to write complex\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# overload signatures for spec_to_mimebundle based on `format`\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspec_to_mimebundle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_div\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_div\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/llm-sbi/.venv/lib/python3.11/site-packages/altair/utils/mimebundle.py:70\u001b[0m, in \u001b[0;36mspec_to_mimebundle\u001b[0;34m(spec, format, mode, vega_version, vegaembed_version, vegalite_version, embed_options, engine, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m internal_mode: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvega-lite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvega\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_vegafusion():\n\u001b[0;32m---> 70\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_with_vegafusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     internal_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvega\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Default to the embed options set by alt.renderers.set_embed_options\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/llm-sbi/.venv/lib/python3.11/site-packages/altair/utils/_vegafusion_data.py:163\u001b[0m, in \u001b[0;36mcompile_with_vegafusion\u001b[0;34m(vegalite_spec)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Pre-evaluate transforms in vega spec with vegafusion\u001b[39;00m\n\u001b[1;32m    162\u001b[0m row_limit \u001b[38;5;241m=\u001b[39m data_transformers\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_rows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 163\u001b[0m transformed_vega_spec, warnings \u001b[38;5;241m=\u001b[39m \u001b[43mvf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transform_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvega_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_local_tz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43minline_datasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Check from row limit warning and convert to MaxRowsError\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m warning \u001b[38;5;129;01min\u001b[39;00m warnings:\n",
      "File \u001b[0;32m~/projects/llm-sbi/.venv/lib/python3.11/site-packages/vegafusion/runtime.py:367\u001b[0m, in \u001b[0;36mVegaFusionRuntime.pre_transform_spec\u001b[0;34m(self, spec, local_tz, default_input_tz, row_limit, preserve_interactivity, inline_datasets, keep_signals, keep_datasets, data_encoding_threshold, data_encoding_format)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_encoding_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m         new_spec, warnings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedded_runtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transform_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_tz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdefault_input_tz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_input_tz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreserve_interactivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_interactivity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43minline_datasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimported_inline_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_signals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_datasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;66;03m# Use pre_transform_extract to extract large datasets\u001b[39;00m\n\u001b[1;32m    379\u001b[0m         new_spec, datasets, warnings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedded_runtime\u001b[38;5;241m.\u001b[39mpre_transform_extract(\n\u001b[1;32m    380\u001b[0m             spec,\n\u001b[1;32m    381\u001b[0m             local_tz\u001b[38;5;241m=\u001b[39mlocal_tz,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m             keep_datasets\u001b[38;5;241m=\u001b[39mkeep_datasets,\n\u001b[1;32m    389\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Expression compilation error: No signal named SelectedToken in evaluation scope\n    Context[0]: Failed to get node value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "\n",
    "def attn_viz(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns, width, scale):\n",
    "    \"\"\"\n",
    "    prompt_tokens should be the Tensor of token IDs from the prompt, 1 x (prompt len)\n",
    "    generated_sequence should be the raw sequence from huggingface .generate(), including both prompt and generated tokens.\n",
    "    generated_token_avg_attns should be a sequence with len(generated tokens), where each element is a sequence with len(tokens through generated token, including prompt)\n",
    "    \"\"\"\n",
    "    df = _prepare_attn_df(prompt_tokens, generated_sequence_tokens,\n",
    "                          generated_token_avg_attns)\n",
    "    _show_attn_chart(df, width, scale)\n",
    "\n",
    "\n",
    "def _printable_token_text(token):\n",
    "    return tokenizer.decode(token).replace('\\n', '\\\\n')\n",
    "\n",
    "\n",
    "def _prepare_attn_df(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns):\n",
    "    inspect_shape('generated_sequence_tokens', generated_sequence_tokens)\n",
    "    inspect_shape('list(generated_sequence_tokens)',\n",
    "                  list(generated_sequence_tokens))\n",
    "    rows = []\n",
    "\n",
    "    num_prompt_tokens = prompt_tokens.shape[1]\n",
    "\n",
    "    # TODO: Make this much more efficient.\n",
    "    # - Tensor iteration\n",
    "    # - Don't both generating rows for attn_from a prompt token\n",
    "    for token_idx, token_id_tensor in enumerate(generated_sequence_tokens):\n",
    "        token_id = token_id_tensor.item()\n",
    "        for attn_from_idx in range(len(generated_sequence_tokens)):\n",
    "            if attn_from_idx < num_prompt_tokens:\n",
    "                attn = 0\n",
    "            else:\n",
    "                generated_idx = attn_from_idx - num_prompt_tokens\n",
    "                generated_token_avg_attn = generated_token_avg_attns[generated_idx]\n",
    "                if token_idx < generated_token_avg_attn.shape[0]:\n",
    "                    attn = generated_token_avg_attn[token_idx].item()\n",
    "                else:  # If this token is in the future it cannot receive attention from a past token\n",
    "                    attn = 0\n",
    "\n",
    "            rows.append({\n",
    "                'token_idx': token_idx,\n",
    "                'token_id': token_id,\n",
    "                'token_text': _printable_token_text(token_id),\n",
    "                'attn_from_idx': attn_from_idx,\n",
    "                'attn': attn\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    display(df)\n",
    "\n",
    "    # for source_token_idx, generated_token_avg_attn in enumerate(generated_token_avg_attns):\n",
    "    #     for target_token_idx, target_token_attn in enumerate(list(generated_token_avg_attn)):\n",
    "    #         rows.append({\n",
    "    #             'source_token_idx': source_token_idx,\n",
    "    #             'source_token_id': generated_sequence_tokens\n",
    "    #         })\n",
    "    # df = pd.DataFrame({'token_id': generated_sequence_tokens, 'token_text': map(\n",
    "    #     _printable_token_text, generated_sequence_tokens)})\n",
    "\n",
    "    # attn_from_cols = [f'token_attn_from_{i}' for i in range(\n",
    "    #     len(generated_sequence_tokens))]\n",
    "    # df = df.reindex(columns=['token_id', 'token_text'] +\n",
    "    #                 attn_from_cols, fill_value=0.0)\n",
    "    # df['token_idx'] = df.index\n",
    "    # df['token_attn_col'] = df['token_idx'].apply(\n",
    "    #     lambda idx: 'token_attn_col_{}'.format(idx))\n",
    "\n",
    "    # # Generated tokens give attention to preceding tokens\n",
    "    # # TODO: Make this more efficient, maybe with an entire matrix instead of setting each vector?\n",
    "    # for token_idx, generated_token_avg_attn in enumerate(generated_token_avg_attns):\n",
    "    #     # inspect_shape('generated_token_avg_attn', generated_token_avg_attn)\n",
    "    #     # print('prompt tokens shape', prompt_tokens.shape)\n",
    "    #     absolute_token_idx = prompt_tokens.shape[1] + token_idx\n",
    "    #     df.loc[:len(generated_token_avg_attn) - 1,\n",
    "    #            f'token_attn_from_{absolute_token_idx}'] = list(generated_token_avg_attn)\n",
    "    # # display(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _show_attn_chart(df, width, scale):\n",
    "    \"\"\"\n",
    "    df should have columns:\n",
    "      token_idx: absolute index of token within sequence, including prompt tokens\n",
    "      token_id: int representation of token\n",
    "      token_text: string representation of token\n",
    "      attn_from_idx: \n",
    "      attn: \n",
    "    width in pixels, of total visualization\n",
    "    scale in float, 1.0 is normal scale (48px x 24px boxes)\n",
    "    \"\"\"\n",
    "\n",
    "    chart_df = df.copy()\n",
    "\n",
    "    # Add x and y information to the tokens based on the calculated number of tokens that can fit the given width\n",
    "    tokens_w = width // (scale * 48)\n",
    "    chart_df['x'] = chart_df['token_idx'].apply(lambda idx: idx % tokens_w)\n",
    "    chart_df['y'] = chart_df['token_idx'].apply(lambda idx: idx // tokens_w)\n",
    "\n",
    "    # display(chart_df)\n",
    "\n",
    "    selected_token = alt.selection_point(\n",
    "        on='mouseover', name='SelectedToken', fields=['token_idx'], empty=False)\n",
    "\n",
    "    selected_token_from = selected_token.project()\n",
    "\n",
    "    base = alt.Chart(chart_df).encode(\n",
    "        x=alt.X('x:N').title('').axis(labels=False, ticks=False),\n",
    "        y=alt.Y('y:N').title('').axis(labels=False, ticks=False)\n",
    "    ).properties(\n",
    "        width=width,\n",
    "        # Number of columns times height of each box\n",
    "        height=(len(df.index) // tokens_w) * (scale * 24)\n",
    "        # ).transform_calculate(\n",
    "        #     selected_attn=f'datum[{selected_token.name}]'\n",
    "        # ).add_params(\n",
    "        #     selected_token\n",
    "    )\n",
    "\n",
    "    highlight = base.mark_rect().encode(\n",
    "        color=alt.Color('selected_attn:Q').scale(\n",
    "            scheme='tealblues').legend(None)\n",
    "        # color=alt.condition(\n",
    "        #     alt.datum.y > selected_token.y,\n",
    "        #     alt.value('red'),\n",
    "        #     alt.value('lightgray')\n",
    "        # )\n",
    "        # ).transform_calculate(\n",
    "        #     selected_attn=f'datum[{selected_token.name}]'\n",
    "        # ).add_params(\n",
    "        #     highlight_param,\n",
    "        #     selected_token\n",
    "    ).transform_filter(\n",
    "        alt.selected_token\n",
    "    )\n",
    "\n",
    "    text = base.mark_text(baseline='middle', fontSize=(scale * 10)).encode(\n",
    "        text=alt.condition(selected_token, alt.value('###'), 'token_text:N')\n",
    "    ).add_params(\n",
    "        selected_token\n",
    "    )\n",
    "\n",
    "    display(highlight + text)\n",
    "\n",
    "\n",
    "def average_token_attn(token_attn):\n",
    "    # TODO: Make this more efficient using torch tensor operations instead of splitting lists\n",
    "    layer_avg_attns = []\n",
    "    for layer_idx, layer_attn in enumerate(token_attn):\n",
    "        # print('layer_idx', layer_idx)\n",
    "        # Take mean across model's attention heads\n",
    "        layer_avg_attn = layer_attn.squeeze(0).mean(dim=0)\n",
    "        # inspect_shape('layer_avg_attn', layer_avg_attn)\n",
    "        layer_avg_attn_cleaned = torch.concat([\n",
    "            torch.tensor([0.]),  # First entry is null attention, set it to 0\n",
    "            # Remove all tokens except the most recent (since the first token generated has entire prompts' worth of attention generated) and remove the first entry\n",
    "            layer_avg_attn[-1][1:],\n",
    "            torch.tensor([0.]),  # Add a 0 for the current token itself\n",
    "        ])\n",
    "        # inspect_shape('layer_avg_attn_cleaned', layer_avg_attn_cleaned)\n",
    "        layer_avg_attn_normalized = layer_avg_attn_cleaned / layer_avg_attn_cleaned.sum()\n",
    "        # inspect_shape('layer_avg_attn_normalized', layer_avg_attn_normalized)\n",
    "        layer_avg_attns.append(layer_avg_attn_normalized)\n",
    "    # inspect_shape('layer_avg_attns', layer_attn)\n",
    "    return torch.stack(layer_avg_attns).mean(dim=0)\n",
    "\n",
    "\n",
    "def generate_and_attn(prompt):\n",
    "    prompt_tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    # inspect_shape('tokens', prompt_tokens)\n",
    "\n",
    "    outputs = model.generate(prompt_tokens, max_new_tokens=48,\n",
    "                             output_attentions=True, return_dict_in_generate=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    # inspect_shape('outputs', outputs)\n",
    "    # token_avg_attn = average_token_attn(outputs.attentions[1])\n",
    "    # inspect_shape('token_avg_attn', token_avg_attn)\n",
    "    # print(token_avg_attn)\n",
    "    # for token in outputs.sequences[0]:\n",
    "    #     print(token, tokenizer.decode(token), end=', ')\n",
    "    generated_token_avg_attns = list(map(\n",
    "        average_token_attn, outputs.attentions))\n",
    "    inspect_shape('generated_token_avg_attns', generated_token_avg_attns)\n",
    "\n",
    "    # inspect_shape('outputs.sequences[0]', outputs.sequences[0])\n",
    "    attn_viz(prompt_tokens,\n",
    "             outputs.sequences[0], generated_token_avg_attns, 800, 2.0)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of \"one world, one dream\". The torch relay took place over 45km with 18 total runners.\n",
    "\n",
    "Q: What was the theme?\n",
    "A:\n",
    "\"\"\".strip()\n",
    "\n",
    "generate_and_attn(PROMPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
