{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'the dog walked the man to the park'\n",
    "\n",
    "# tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "# print(tokens)\n",
    "# print(tokenizer.tokenize(prompt))\n",
    "# outputs = model(tokens, output_attentions=True)\n",
    "# print(len(outputs.attentions))\n",
    "# outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import model_view, head_view\n",
    "\n",
    "# model_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "# head_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal:\n",
    "# 1. Take a prompt.\n",
    "# 2. Generate an answer to the question.\n",
    "# 3. Quantify where attention was paid on each token of the answer.\n",
    "# 4. Visualize that attention. Visualization should work for long passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_shape(name, obj):\n",
    "    # Since this is the top-level inspect call, print the name of the variable.\n",
    "    print(f'{name}: ', end='')\n",
    "    _inspect_shape(obj, len(name) + 2)  # + 2 for ': '\n",
    "\n",
    "\n",
    "def _class_name(obj):\n",
    "    return type(obj).__name__\n",
    "\n",
    "\n",
    "def _inspect_shape(obj, indent):\n",
    "    \"\"\" \n",
    "    Print the given obj. Does not print preceding name or preceding indentation.\n",
    "    Param `indent` should be the indentation level where this obj is being printed. Will be used to print nested properties if necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base Cases (sequences with known-typed contents, tensors, other objects or primitives)\n",
    "    if isinstance(obj, str):\n",
    "        print(f'{_class_name(obj)}[len={len(obj)}]')\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(f'{_class_name(obj)}[shape={list(obj.shape)}, sum={obj.sum()}]')\n",
    "    else:\n",
    "\n",
    "        # Dict where contents should be recursively inspected\n",
    "        try:\n",
    "            print(f'{_class_name(obj)}[len={len(obj.items())}]')\n",
    "            for key, val in obj.items():\n",
    "                print(' ' * (indent + 2) + f'{key}: ', end='')\n",
    "                _inspect_shape(val, indent + len(key) + 2)  # + 2 for ': '\n",
    "        except (TypeError, AttributeError):\n",
    "            # Sequence where contents should be recursively inspected\n",
    "            try:\n",
    "                print(f'{_class_name(obj)}[len={len(obj)}]')\n",
    "                for i in range(min(len(obj), 3)):\n",
    "                    print(' ' * (indent + 2) + f'{i}: ', end='')\n",
    "                    # + 2 for ': '\n",
    "                    _inspect_shape(obj[i], indent + len(str(i)) + 2)\n",
    "            except (TypeError, AttributeError):\n",
    "                # Default case if nothing else works\n",
    "                print(f'{_class_name(obj)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Tensor[shape=[1, 60], sum=254623]\n",
      "outputs: GreedySearchDecoderOnlyOutput[len=3]\n",
      "           sequences: Tensor[shape=[1, 108], sum=454166]\n",
      "           attentions: tuple[len=48]\n",
      "                       0: tuple[len=12]\n",
      "                          0: Tensor[shape=[1, 12, 60, 60], sum=720.0]\n",
      "                          1: Tensor[shape=[1, 12, 60, 60], sum=720.0]\n",
      "                          2: Tensor[shape=[1, 12, 60, 60], sum=720.0]\n",
      "                       1: tuple[len=12]\n",
      "                          0: Tensor[shape=[1, 12, 1, 61], sum=11.999999046325684]\n",
      "                          1: Tensor[shape=[1, 12, 1, 61], sum=12.000000953674316]\n",
      "                          2: Tensor[shape=[1, 12, 1, 61], sum=12.0]\n",
      "                       2: tuple[len=12]\n",
      "                          0: Tensor[shape=[1, 12, 1, 62], sum=12.000000953674316]\n",
      "                          1: Tensor[shape=[1, 12, 1, 62], sum=12.0]\n",
      "                          2: Tensor[shape=[1, 12, 1, 62], sum=11.999999046325684]\n",
      "           past_key_values: tuple[len=12]\n",
      "                            0: tuple[len=2]\n",
      "                               0: Tensor[shape=[1, 12, 107, 64], sum=-2187.402099609375]\n",
      "                               1: Tensor[shape=[1, 12, 107, 64], sum=570.8171997070312]\n",
      "                            1: tuple[len=2]\n",
      "                               0: Tensor[shape=[1, 12, 107, 64], sum=-2905.125]\n",
      "                               1: Tensor[shape=[1, 12, 107, 64], sum=799.8795166015625]\n",
      "                            2: tuple[len=2]\n",
      "                               0: Tensor[shape=[1, 12, 107, 64], sum=-897.89990234375]\n",
      "                               1: Tensor[shape=[1, 12, 107, 64], sum=-732.2595825195312]\n",
      "generated_token_avg_attns: list[len=48]\n",
      "                             0: Tensor[shape=[61], sum=1.0]\n",
      "                             1: Tensor[shape=[62], sum=1.0]\n",
      "                             2: Tensor[shape=[63], sum=1.0]\n",
      "outputs.sequences[0]: Tensor[shape=[108], sum=454166]\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_attn_from_0</th>\n",
       "      <th>token_attn_from_1</th>\n",
       "      <th>token_attn_from_2</th>\n",
       "      <th>token_attn_from_3</th>\n",
       "      <th>token_attn_from_4</th>\n",
       "      <th>token_attn_from_5</th>\n",
       "      <th>token_attn_from_6</th>\n",
       "      <th>token_attn_from_7</th>\n",
       "      <th>...</th>\n",
       "      <th>token_attn_from_99</th>\n",
       "      <th>token_attn_from_100</th>\n",
       "      <th>token_attn_from_101</th>\n",
       "      <th>token_attn_from_102</th>\n",
       "      <th>token_attn_from_103</th>\n",
       "      <th>token_attn_from_104</th>\n",
       "      <th>token_attn_from_105</th>\n",
       "      <th>token_attn_from_106</th>\n",
       "      <th>token_attn_from_107</th>\n",
       "      <th>token_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464</td>\n",
       "      <td>The</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3648</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10216</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14935</td>\n",
       "      <td>Olympics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28034</td>\n",
       "      <td>torch</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.011573</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>28034</td>\n",
       "      <td>torch</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078872</td>\n",
       "      <td>0.090071</td>\n",
       "      <td>0.041559</td>\n",
       "      <td>0.034214</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>24248</td>\n",
       "      <td>relay</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069411</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>0.060633</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1718</td>\n",
       "      <td>took</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082932</td>\n",
       "      <td>0.118218</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1295</td>\n",
       "      <td>place</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067498</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>625</td>\n",
       "      <td>over</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     token_id token_text  token_attn_from_0  token_attn_from_1  \\\n",
       "0         464        The                0.0                0.0   \n",
       "1        3648       2008                0.0                0.0   \n",
       "2       10216     Summer                0.0                0.0   \n",
       "3       14935   Olympics                0.0                0.0   \n",
       "4       28034      torch                0.0                0.0   \n",
       "..        ...        ...                ...                ...   \n",
       "103     28034      torch                0.0                0.0   \n",
       "104     24248      relay                0.0                0.0   \n",
       "105      1718       took                0.0                0.0   \n",
       "106      1295      place                0.0                0.0   \n",
       "107       625       over                0.0                0.0   \n",
       "\n",
       "     token_attn_from_2  token_attn_from_3  token_attn_from_4  \\\n",
       "0                  0.0                0.0                0.0   \n",
       "1                  0.0                0.0                0.0   \n",
       "2                  0.0                0.0                0.0   \n",
       "3                  0.0                0.0                0.0   \n",
       "4                  0.0                0.0                0.0   \n",
       "..                 ...                ...                ...   \n",
       "103                0.0                0.0                0.0   \n",
       "104                0.0                0.0                0.0   \n",
       "105                0.0                0.0                0.0   \n",
       "106                0.0                0.0                0.0   \n",
       "107                0.0                0.0                0.0   \n",
       "\n",
       "     token_attn_from_5  token_attn_from_6  token_attn_from_7  ...  \\\n",
       "0                  0.0                0.0                0.0  ...   \n",
       "1                  0.0                0.0                0.0  ...   \n",
       "2                  0.0                0.0                0.0  ...   \n",
       "3                  0.0                0.0                0.0  ...   \n",
       "4                  0.0                0.0                0.0  ...   \n",
       "..                 ...                ...                ...  ...   \n",
       "103                0.0                0.0                0.0  ...   \n",
       "104                0.0                0.0                0.0  ...   \n",
       "105                0.0                0.0                0.0  ...   \n",
       "106                0.0                0.0                0.0  ...   \n",
       "107                0.0                0.0                0.0  ...   \n",
       "\n",
       "     token_attn_from_99  token_attn_from_100  token_attn_from_101  \\\n",
       "0              0.000000             0.000000             0.000000   \n",
       "1              0.003128             0.002640             0.003904   \n",
       "2              0.001960             0.002374             0.002796   \n",
       "3              0.002512             0.004261             0.005024   \n",
       "4              0.001940             0.006182             0.004596   \n",
       "..                  ...                  ...                  ...   \n",
       "103            0.000000             0.000000             0.000000   \n",
       "104            0.000000             0.000000             0.000000   \n",
       "105            0.000000             0.000000             0.000000   \n",
       "106            0.000000             0.000000             0.000000   \n",
       "107            0.000000             0.000000             0.000000   \n",
       "\n",
       "     token_attn_from_102  token_attn_from_103  token_attn_from_104  \\\n",
       "0               0.000000             0.000000             0.000000   \n",
       "1               0.008104             0.016115             0.003378   \n",
       "2               0.004580             0.005535             0.002284   \n",
       "3               0.005697             0.007323             0.005890   \n",
       "4               0.004449             0.009333             0.011573   \n",
       "..                   ...                  ...                  ...   \n",
       "103             0.000000             0.000000             0.078872   \n",
       "104             0.000000             0.000000             0.000000   \n",
       "105             0.000000             0.000000             0.000000   \n",
       "106             0.000000             0.000000             0.000000   \n",
       "107             0.000000             0.000000             0.000000   \n",
       "\n",
       "     token_attn_from_105  token_attn_from_106  token_attn_from_107  token_idx  \n",
       "0               0.000000             0.000000             0.000000          0  \n",
       "1               0.003049             0.004015             0.003860          1  \n",
       "2               0.001731             0.001930             0.002719          2  \n",
       "3               0.003913             0.003842             0.003923          3  \n",
       "4               0.005620             0.003233             0.004239          4  \n",
       "..                   ...                  ...                  ...        ...  \n",
       "103             0.090071             0.041559             0.034214        103  \n",
       "104             0.069411             0.087705             0.060633        104  \n",
       "105             0.000000             0.082932             0.118218        105  \n",
       "106             0.000000             0.000000             0.067498        106  \n",
       "107             0.000000             0.000000             0.000000        107  \n",
       "\n",
       "[108 rows x 111 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "_show_attn_chart() missing 1 required positional argument: 'scale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 133\u001b[0m\n\u001b[1;32m    122\u001b[0m     attn_viz(prompt_tokens,\n\u001b[1;32m    123\u001b[0m              outputs\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m], generated_token_avg_attns, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    126\u001b[0m PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124mThe 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone world, one dream\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. The torch relay took place over 45km with 18 total runners.\u001b[39m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[38;5;124mQ: What was the theme?\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124mA:\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m--> 133\u001b[0m \u001b[43mgenerate_and_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[153], line 122\u001b[0m, in \u001b[0;36mgenerate_and_attn\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m    119\u001b[0m inspect_shape(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_token_avg_attns\u001b[39m\u001b[38;5;124m'\u001b[39m, generated_token_avg_attns)\n\u001b[1;32m    121\u001b[0m inspect_shape(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs.sequences[0]\u001b[39m\u001b[38;5;124m'\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 122\u001b[0m \u001b[43mattn_viz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m         \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_token_avg_attns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[153], line 13\u001b[0m, in \u001b[0;36mattn_viz\u001b[0;34m(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns, width, scale)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mprompt_tokens should be the Tensor of token IDs from the prompt, 1 x (prompt len)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mgenerated_sequence should be the raw sequence from huggingface .generate(), including both prompt and generated tokens.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mgenerated_token_avg_attns should be a sequence with len(generated tokens), where each element is a sequence with len(tokens through generated token, including prompt)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m _prepare_attn_df(prompt_tokens, generated_sequence_tokens,\n\u001b[1;32m     12\u001b[0m                       generated_token_avg_attns)\n\u001b[0;32m---> 13\u001b[0m \u001b[43m_show_attn_chart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: _show_attn_chart() missing 1 required positional argument: 'scale'"
     ]
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def attn_viz(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns, width, scale):\n",
    "    \"\"\"\n",
    "    prompt_tokens should be the Tensor of token IDs from the prompt, 1 x (prompt len)\n",
    "    generated_sequence should be the raw sequence from huggingface .generate(), including both prompt and generated tokens.\n",
    "    generated_token_avg_attns should be a sequence with len(generated tokens), where each element is a sequence with len(tokens through generated token, including prompt)\n",
    "    \"\"\"\n",
    "    df = _prepare_attn_df(prompt_tokens, generated_sequence_tokens,\n",
    "                          generated_token_avg_attns)\n",
    "    _show_attn_chart(df, width, scale)\n",
    "\n",
    "\n",
    "def _printable_token_text(token):\n",
    "    return tokenizer.decode(token).replace('\\n', '\\\\n')\n",
    "\n",
    "\n",
    "def _prepare_attn_df(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns):\n",
    "    df = pd.DataFrame({'token_id': generated_sequence_tokens, 'token_text': map(\n",
    "        _printable_token_text, generated_sequence_tokens)})\n",
    "\n",
    "    attn_from_cols = [f'token_attn_from_{i}' for i in range(\n",
    "        len(generated_sequence_tokens))]\n",
    "    df = df.reindex(columns=['token_id', 'token_text'] +\n",
    "                    attn_from_cols, fill_value=0.0)\n",
    "    df['token_idx'] = df.index\n",
    "\n",
    "    # Generated tokens give attention to preceding tokens\n",
    "    # TODO: Make this more efficient, maybe with an entire matrix instead of setting each vector?\n",
    "    for token_idx, generated_token_avg_attn in enumerate(generated_token_avg_attns):\n",
    "        # inspect_shape('generated_token_avg_attn', generated_token_avg_attn)\n",
    "        print('prompt tokens shape', prompt_tokens.shape)\n",
    "        absolute_token_idx = prompt_tokens.shape[1] + token_idx\n",
    "        df.loc[:len(generated_token_avg_attn) - 1,\n",
    "               f'token_attn_from_{absolute_token_idx}'] = list(generated_token_avg_attn)\n",
    "    display(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _show_attn_chart(df, width, scale):\n",
    "    \"\"\"\n",
    "    df should have columns:\n",
    "      token_idx: absolute index of token within sequence, including prompt tokens\n",
    "      token_id: int representation of token\n",
    "      token_text: string representation of token\n",
    "      token_attn_from_{i}: float representing the attention placed on this token by the token with index i in the sequence\n",
    "    width in pixels, of total visualization\n",
    "    scale in float, 1.0 is normal scale (48px x 24px boxes)\n",
    "    \"\"\"\n",
    "\n",
    "    chart_df = df.copy()\n",
    "\n",
    "    # Add x and y information to the tokens based on the calculated number of tokens that can fit the given width\n",
    "    tokens_w = width // (scale * 48)\n",
    "    chart_df['x'] = chart_df['token_idx'].apply(lambda idx: idx % tokens_w)\n",
    "    chart_df['y'] = chart_df['token_idx'].apply(lambda idx: idx // tokens_w)\n",
    "\n",
    "    display(chart_df)\n",
    "\n",
    "    base = alt.Chart(chart_df).encode(\n",
    "        x=alt.X('x:N').title('').axis(labels=False, ticks=False),\n",
    "        y=alt.Y('y:N').title('').axis(labels=False, ticks=False)\n",
    "    ).properties(\n",
    "        width=width,\n",
    "        # Number of columns times height of each box\n",
    "        height=(len(df.index) // tokens_w) * (scale * 24)\n",
    "    )\n",
    "\n",
    "    highlight = base.mark_rect().encode(\n",
    "        alt.Color('token_attn_from_100:Q').scale(\n",
    "            scheme='tealblues').legend(None)\n",
    "    )\n",
    "\n",
    "    text = base.mark_text(baseline='middle').encode(\n",
    "        text='token_text:N'\n",
    "    )\n",
    "\n",
    "    display(highlight + text)\n",
    "\n",
    "\n",
    "def average_token_attn(token_attn):\n",
    "    # TODO: Make this more efficient using torch tensor operations instead of splitting lists\n",
    "    layer_avg_attns = []\n",
    "    for layer_idx, layer_attn in enumerate(token_attn):\n",
    "        # print('layer_idx', layer_idx)\n",
    "        # Take mean across model's attention heads\n",
    "        layer_avg_attn = layer_attn.squeeze(0).mean(dim=0)\n",
    "        # inspect_shape('layer_avg_attn', layer_avg_attn)\n",
    "        layer_avg_attn_cleaned = torch.concat([\n",
    "            torch.tensor([0.]),  # First entry is null attention, set it to 0\n",
    "            # Remove all tokens except the most recent (since the first token generated has entire prompts' worth of attention generated) and remove the first entry\n",
    "            layer_avg_attn[-1][1:],\n",
    "            torch.tensor([0.]),  # Add a 0 for the current token itself\n",
    "        ])\n",
    "        # inspect_shape('layer_avg_attn_cleaned', layer_avg_attn_cleaned)\n",
    "        layer_avg_attn_normalized = layer_avg_attn_cleaned / layer_avg_attn_cleaned.sum()\n",
    "        # inspect_shape('layer_avg_attn_normalized', layer_avg_attn_normalized)\n",
    "        layer_avg_attns.append(layer_avg_attn_normalized)\n",
    "    # inspect_shape('layer_avg_attns', layer_attn)\n",
    "    return torch.stack(layer_avg_attns).mean(dim=0)\n",
    "\n",
    "\n",
    "def generate_and_attn(prompt):\n",
    "    prompt_tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    inspect_shape('tokens', prompt_tokens)\n",
    "\n",
    "    outputs = model.generate(prompt_tokens, max_new_tokens=48,\n",
    "                             output_attentions=True, return_dict_in_generate=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    inspect_shape('outputs', outputs)\n",
    "    # token_avg_attn = average_token_attn(outputs.attentions[1])\n",
    "    # inspect_shape('token_avg_attn', token_avg_attn)\n",
    "    # print(token_avg_attn)\n",
    "    # for token in outputs.sequences[0]:\n",
    "    #     print(token, tokenizer.decode(token), end=', ')\n",
    "    generated_token_avg_attns = list(map(\n",
    "        average_token_attn, outputs.attentions))\n",
    "    inspect_shape('generated_token_avg_attns', generated_token_avg_attns)\n",
    "\n",
    "    inspect_shape('outputs.sequences[0]', outputs.sequences[0])\n",
    "    attn_viz(prompt_tokens,\n",
    "             outputs.sequences[0], generated_token_avg_attns, 512, 1.0)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of \"one world, one dream\". The torch relay took place over 45km with 18 total runners.\n",
    "\n",
    "Q: What was the theme?\n",
    "A:\n",
    "\"\"\".strip()\n",
    "\n",
    "generate_and_attn(PROMPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
