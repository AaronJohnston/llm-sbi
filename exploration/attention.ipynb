{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'the dog walked the man to the park'\n",
    "\n",
    "# tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "# print(tokens)\n",
    "# print(tokenizer.tokenize(prompt))\n",
    "# outputs = model(tokens, output_attentions=True)\n",
    "# print(len(outputs.attentions))\n",
    "# outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import model_view, head_view\n",
    "\n",
    "# model_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "# head_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal:\n",
    "# 1. Take a prompt.\n",
    "# 2. Generate an answer to the question.\n",
    "# 3. Quantify where attention was paid on each token of the answer.\n",
    "# 4. Visualize that attention. Visualization should work for long passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_shape(name, obj):\n",
    "    # Since this is the top-level inspect call, print the name of the variable.\n",
    "    print(f'{name}: ', end='')\n",
    "    _inspect_shape(obj, len(name) + 2)  # + 2 for ': '\n",
    "\n",
    "\n",
    "def _class_name(obj):\n",
    "    return type(obj).__name__\n",
    "\n",
    "\n",
    "def _inspect_shape(obj, indent):\n",
    "    \"\"\" \n",
    "    Print the given obj. Does not print preceding name or preceding indentation.\n",
    "    Param `indent` should be the indentation level where this obj is being printed. Will be used to print nested properties if necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base Cases (sequences with known-typed contents, tensors, other objects or primitives)\n",
    "    if isinstance(obj, str):\n",
    "        print(f'{_class_name(obj)}[len={len(obj)}]')\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(f'{_class_name(obj)}[shape={list(obj.shape)}, sum={obj.sum()}]')\n",
    "    else:\n",
    "\n",
    "        # Dict where contents should be recursively inspected\n",
    "        try:\n",
    "            print(f'{_class_name(obj)}[len={len(obj.items())}]')\n",
    "            for key, val in obj.items():\n",
    "                print(' ' * (indent + 2) + f'{key}: ', end='')\n",
    "                _inspect_shape(val, indent + len(key) + 2)  # + 2 for ': '\n",
    "        except (TypeError, AttributeError):\n",
    "            # Sequence where contents should be recursively inspected\n",
    "            try:\n",
    "                print(f'{_class_name(obj)}[len={len(obj)}]')\n",
    "                for i in range(min(len(obj), 3)):\n",
    "                    print(' ' * (indent + 2) + f'{i}: ', end='')\n",
    "                    # + 2 for ': '\n",
    "                    _inspect_shape(obj[i], indent + len(str(i)) + 2)\n",
    "            except (TypeError, AttributeError):\n",
    "                # Default case if nothing else works\n",
    "                print(f'{_class_name(obj)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Tensor[shape=[1, 60], sum=254623]\n",
      "outputs: GreedySearchDecoderOnlyOutput[len=3]\n",
      "           sequences: Tensor[shape=[1, 108], sum=454166]\n",
      "           attentions: tuple[len=48]\n",
      "                       0: tuple[len=12]\n",
      "                          0: Tensor[shape=[1, 12, 60, 60], sum=720.0]\n",
      "                          1: Tensor[shape=[1, 12, 60, 60], sum=720.0]\n",
      "                          2: Tensor[shape=[1, 12, 60, 60], sum=720.0]\n",
      "                       1: tuple[len=12]\n",
      "                          0: Tensor[shape=[1, 12, 1, 61], sum=11.999999046325684]\n",
      "                          1: Tensor[shape=[1, 12, 1, 61], sum=12.000000953674316]\n",
      "                          2: Tensor[shape=[1, 12, 1, 61], sum=12.0]\n",
      "                       2: tuple[len=12]\n",
      "                          0: Tensor[shape=[1, 12, 1, 62], sum=12.000000953674316]\n",
      "                          1: Tensor[shape=[1, 12, 1, 62], sum=12.0]\n",
      "                          2: Tensor[shape=[1, 12, 1, 62], sum=11.999999046325684]\n",
      "           past_key_values: tuple[len=12]\n",
      "                            0: tuple[len=2]\n",
      "                               0: Tensor[shape=[1, 12, 107, 64], sum=-2187.402099609375]\n",
      "                               1: Tensor[shape=[1, 12, 107, 64], sum=570.8171997070312]\n",
      "                            1: tuple[len=2]\n",
      "                               0: Tensor[shape=[1, 12, 107, 64], sum=-2905.125]\n",
      "                               1: Tensor[shape=[1, 12, 107, 64], sum=799.8795166015625]\n",
      "                            2: tuple[len=2]\n",
      "                               0: Tensor[shape=[1, 12, 107, 64], sum=-897.89990234375]\n",
      "                               1: Tensor[shape=[1, 12, 107, 64], sum=-732.2595825195312]\n",
      "generated_token_avg_attns: list[len=48]\n",
      "                             0: Tensor[shape=[61], sum=1.0]\n",
      "                             1: Tensor[shape=[62], sum=1.0]\n",
      "                             2: Tensor[shape=[63], sum=1.0]\n",
      "outputs.sequences[0]: Tensor[shape=[108], sum=454166]\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n",
      "prompt tokens shape torch.Size([1, 60])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_attn_from_0</th>\n",
       "      <th>token_attn_from_1</th>\n",
       "      <th>token_attn_from_2</th>\n",
       "      <th>token_attn_from_3</th>\n",
       "      <th>token_attn_from_4</th>\n",
       "      <th>token_attn_from_5</th>\n",
       "      <th>token_attn_from_6</th>\n",
       "      <th>token_attn_from_7</th>\n",
       "      <th>...</th>\n",
       "      <th>token_attn_from_98</th>\n",
       "      <th>token_attn_from_99</th>\n",
       "      <th>token_attn_from_100</th>\n",
       "      <th>token_attn_from_101</th>\n",
       "      <th>token_attn_from_102</th>\n",
       "      <th>token_attn_from_103</th>\n",
       "      <th>token_attn_from_104</th>\n",
       "      <th>token_attn_from_105</th>\n",
       "      <th>token_attn_from_106</th>\n",
       "      <th>token_attn_from_107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464</td>\n",
       "      <td>The</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3648</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.003860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10216</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.002719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14935</td>\n",
       "      <td>Olympics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.003923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28034</td>\n",
       "      <td>torch</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.011573</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.004239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>28034</td>\n",
       "      <td>torch</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078872</td>\n",
       "      <td>0.090071</td>\n",
       "      <td>0.041559</td>\n",
       "      <td>0.034214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>24248</td>\n",
       "      <td>relay</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069411</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>0.060633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1718</td>\n",
       "      <td>took</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082932</td>\n",
       "      <td>0.118218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1295</td>\n",
       "      <td>place</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>625</td>\n",
       "      <td>over</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     token_id token_text  token_attn_from_0  token_attn_from_1  \\\n",
       "0         464        The                0.0                0.0   \n",
       "1        3648       2008                0.0                0.0   \n",
       "2       10216     Summer                0.0                0.0   \n",
       "3       14935   Olympics                0.0                0.0   \n",
       "4       28034      torch                0.0                0.0   \n",
       "..        ...        ...                ...                ...   \n",
       "103     28034      torch                0.0                0.0   \n",
       "104     24248      relay                0.0                0.0   \n",
       "105      1718       took                0.0                0.0   \n",
       "106      1295      place                0.0                0.0   \n",
       "107       625       over                0.0                0.0   \n",
       "\n",
       "     token_attn_from_2  token_attn_from_3  token_attn_from_4  \\\n",
       "0                  0.0                0.0                0.0   \n",
       "1                  0.0                0.0                0.0   \n",
       "2                  0.0                0.0                0.0   \n",
       "3                  0.0                0.0                0.0   \n",
       "4                  0.0                0.0                0.0   \n",
       "..                 ...                ...                ...   \n",
       "103                0.0                0.0                0.0   \n",
       "104                0.0                0.0                0.0   \n",
       "105                0.0                0.0                0.0   \n",
       "106                0.0                0.0                0.0   \n",
       "107                0.0                0.0                0.0   \n",
       "\n",
       "     token_attn_from_5  token_attn_from_6  token_attn_from_7  ...  \\\n",
       "0                  0.0                0.0                0.0  ...   \n",
       "1                  0.0                0.0                0.0  ...   \n",
       "2                  0.0                0.0                0.0  ...   \n",
       "3                  0.0                0.0                0.0  ...   \n",
       "4                  0.0                0.0                0.0  ...   \n",
       "..                 ...                ...                ...  ...   \n",
       "103                0.0                0.0                0.0  ...   \n",
       "104                0.0                0.0                0.0  ...   \n",
       "105                0.0                0.0                0.0  ...   \n",
       "106                0.0                0.0                0.0  ...   \n",
       "107                0.0                0.0                0.0  ...   \n",
       "\n",
       "     token_attn_from_98  token_attn_from_99  token_attn_from_100  \\\n",
       "0              0.000000            0.000000             0.000000   \n",
       "1              0.003052            0.003128             0.002640   \n",
       "2              0.002713            0.001960             0.002374   \n",
       "3              0.006431            0.002512             0.004261   \n",
       "4              0.003024            0.001940             0.006182   \n",
       "..                  ...                 ...                  ...   \n",
       "103            0.000000            0.000000             0.000000   \n",
       "104            0.000000            0.000000             0.000000   \n",
       "105            0.000000            0.000000             0.000000   \n",
       "106            0.000000            0.000000             0.000000   \n",
       "107            0.000000            0.000000             0.000000   \n",
       "\n",
       "     token_attn_from_101  token_attn_from_102  token_attn_from_103  \\\n",
       "0               0.000000             0.000000             0.000000   \n",
       "1               0.003904             0.008104             0.016115   \n",
       "2               0.002796             0.004580             0.005535   \n",
       "3               0.005024             0.005697             0.007323   \n",
       "4               0.004596             0.004449             0.009333   \n",
       "..                   ...                  ...                  ...   \n",
       "103             0.000000             0.000000             0.000000   \n",
       "104             0.000000             0.000000             0.000000   \n",
       "105             0.000000             0.000000             0.000000   \n",
       "106             0.000000             0.000000             0.000000   \n",
       "107             0.000000             0.000000             0.000000   \n",
       "\n",
       "     token_attn_from_104  token_attn_from_105  token_attn_from_106  \\\n",
       "0               0.000000             0.000000             0.000000   \n",
       "1               0.003378             0.003049             0.004015   \n",
       "2               0.002284             0.001731             0.001930   \n",
       "3               0.005890             0.003913             0.003842   \n",
       "4               0.011573             0.005620             0.003233   \n",
       "..                   ...                  ...                  ...   \n",
       "103             0.078872             0.090071             0.041559   \n",
       "104             0.000000             0.069411             0.087705   \n",
       "105             0.000000             0.000000             0.082932   \n",
       "106             0.000000             0.000000             0.000000   \n",
       "107             0.000000             0.000000             0.000000   \n",
       "\n",
       "     token_attn_from_107  \n",
       "0               0.000000  \n",
       "1               0.003860  \n",
       "2               0.002719  \n",
       "3               0.003923  \n",
       "4               0.004239  \n",
       "..                   ...  \n",
       "103             0.034214  \n",
       "104             0.060633  \n",
       "105             0.118218  \n",
       "106             0.067498  \n",
       "107             0.000000  \n",
       "\n",
       "[108 rows x 110 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def attn_viz(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns):\n",
    "    \"\"\"\n",
    "    prompt_tokens should be the Tensor of token IDs from the prompt, 1 x (prompt len)\n",
    "    generated_sequence should be the raw sequence from huggingface .generate(), including both prompt and generated tokens.\n",
    "    generated_token_avg_attns should be a sequence with len(generated tokens), where each element is a sequence with len(tokens through generated token, including prompt)\n",
    "    \"\"\"\n",
    "    df = _prepare_attn_df(prompt_tokens, generated_sequence_tokens,\n",
    "                          generated_token_avg_attns)\n",
    "    _show_attn_chart(df)\n",
    "\n",
    "\n",
    "def _prepare_attn_df(prompt_tokens, generated_sequence_tokens, generated_token_avg_attns):\n",
    "    df = pd.DataFrame({'token_id': generated_sequence_tokens, 'token_text': map(\n",
    "        tokenizer.decode, generated_sequence_tokens)})\n",
    "    attn_from_cols = [f'token_attn_from_{i}' for i in range(\n",
    "        len(generated_sequence_tokens))]\n",
    "    df = df.reindex(columns=['token_id', 'token_text'] +\n",
    "                    attn_from_cols, fill_value=0.0)\n",
    "\n",
    "    # Generated tokens give attention to preceding tokens\n",
    "    # TODO: Make this more efficient, maybe with an entire matrix instead of setting each vector?\n",
    "    for token_idx, generated_token_avg_attn in enumerate(generated_token_avg_attns):\n",
    "        # inspect_shape('generated_token_avg_attn', generated_token_avg_attn)\n",
    "        print('prompt tokens shape', prompt_tokens.shape)\n",
    "        absolute_token_idx = prompt_tokens.shape[1] + token_idx\n",
    "        df.loc[:len(generated_token_avg_attn) - 1,\n",
    "               f'token_attn_from_{absolute_token_idx}'] = list(generated_token_avg_attn)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def _show_attn_chart(df):\n",
    "    \"\"\"\n",
    "    df should have columns:\n",
    "      token_id: int representation of token\n",
    "      token_text: string representation of token\n",
    "      token_attn_from_{i}: float representing the attention placed on this token by the token with index i in the sequence\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def average_token_attn(token_attn):\n",
    "    # TODO: Make this more efficient using torch tensor operations instead of splitting lists\n",
    "    layer_avg_attns = []\n",
    "    for layer_idx, layer_attn in enumerate(token_attn):\n",
    "        # print('layer_idx', layer_idx)\n",
    "        # Take mean across model's attention heads\n",
    "        layer_avg_attn = layer_attn.squeeze(0).mean(dim=0)\n",
    "        # inspect_shape('layer_avg_attn', layer_avg_attn)\n",
    "        layer_avg_attn_cleaned = torch.concat([\n",
    "            torch.tensor([0.]),  # First entry is null attention, set it to 0\n",
    "            # Remove all tokens except the most recent (since the first token generated has entire prompts' worth of attention generated) and remove the first entry\n",
    "            layer_avg_attn[-1][1:],\n",
    "            torch.tensor([0.]),  # Add a 0 for the current token itself\n",
    "        ])\n",
    "        # inspect_shape('layer_avg_attn_cleaned', layer_avg_attn_cleaned)\n",
    "        layer_avg_attn_normalized = layer_avg_attn_cleaned / layer_avg_attn_cleaned.sum()\n",
    "        # inspect_shape('layer_avg_attn_normalized', layer_avg_attn_normalized)\n",
    "        layer_avg_attns.append(layer_avg_attn_normalized)\n",
    "    # inspect_shape('layer_avg_attns', layer_attn)\n",
    "    return torch.stack(layer_avg_attns).mean(dim=0)\n",
    "\n",
    "\n",
    "def generate_and_attn(prompt):\n",
    "    prompt_tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    inspect_shape('tokens', prompt_tokens)\n",
    "\n",
    "    outputs = model.generate(prompt_tokens, max_new_tokens=48,\n",
    "                             output_attentions=True, return_dict_in_generate=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    inspect_shape('outputs', outputs)\n",
    "    # token_avg_attn = average_token_attn(outputs.attentions[1])\n",
    "    # inspect_shape('token_avg_attn', token_avg_attn)\n",
    "    # print(token_avg_attn)\n",
    "    # for token in outputs.sequences[0]:\n",
    "    #     print(token, tokenizer.decode(token), end=', ')\n",
    "    generated_token_avg_attns = list(map(\n",
    "        average_token_attn, outputs.attentions))\n",
    "    inspect_shape('generated_token_avg_attns', generated_token_avg_attns)\n",
    "\n",
    "    inspect_shape('outputs.sequences[0]', outputs.sequences[0])\n",
    "    attn_viz(prompt_tokens, outputs.sequences[0], generated_token_avg_attns)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of \"one world, one dream\". The torch relay took place over 45km with 18 total runners.\n",
    "\n",
    "Q: What was the theme?\n",
    "A:\n",
    "\"\"\".strip()\n",
    "\n",
    "generate_and_attn(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1826, 0.3651, 0.5477, 0.7303])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2, 3, 4])\n",
    "torch.nn.functional.normalize(a, dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
