{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'the dog walked the man to the park'\n",
    "\n",
    "# tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "# print(tokens)\n",
    "# print(tokenizer.tokenize(prompt))\n",
    "# outputs = model(tokens, output_attentions=True)\n",
    "# print(len(outputs.attentions))\n",
    "# outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import model_view, head_view\n",
    "\n",
    "# model_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "# head_view(outputs.attentions, tokenizer.convert_ids_to_tokens(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal:\n",
    "# 1. Take a prompt.\n",
    "# 2. Generate an answer to the question.\n",
    "# 3. Quantify where attention was paid on each token of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_shape(name, obj):\n",
    "    # Since this is the top-level inspect call, print the name of the variable.\n",
    "    print(f'{name}: ', end='')\n",
    "    _inspect_shape(obj, 0)\n",
    "\n",
    "\n",
    "def _inspect_shape(obj, indent):\n",
    "    # Print given the current indent\n",
    "\n",
    "    # Sequence where contents should be recursively inspected\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        print(' ' * indent + f'{type(obj)}[len={len(obj)}]')\n",
    "        for i in range(min(len(obj), 3)):\n",
    "            _inspect_shape(obj[i], indent + 2)\n",
    "\n",
    "    # Base Cases (sequences with known-typed contents, tensors, other objects or primitives)\n",
    "    elif isinstance(obj, str):\n",
    "        print(' ' * indent + f'str[len={len(obj)}]')\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(' ' * indent + f'tensor[shape={obj.shape}, sum={obj.sum()}]')\n",
    "    else:\n",
    "        print(' ' * indent + f'{type(obj)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: tensor[shape=torch.Size([1, 60]), sum=254623]\n",
      "outputs.attentions: <class 'tuple'>[len=48]\n",
      "  <class 'tuple'>[len=12]\n",
      "    tensor[shape=torch.Size([1, 12, 60, 60]), sum=720.0]\n",
      "    tensor[shape=torch.Size([1, 12, 60, 60]), sum=720.0]\n",
      "    tensor[shape=torch.Size([1, 12, 60, 60]), sum=720.0]\n",
      "  <class 'tuple'>[len=12]\n",
      "    tensor[shape=torch.Size([1, 12, 1, 61]), sum=11.999999046325684]\n",
      "    tensor[shape=torch.Size([1, 12, 1, 61]), sum=12.000000953674316]\n",
      "    tensor[shape=torch.Size([1, 12, 1, 61]), sum=12.0]\n",
      "  <class 'tuple'>[len=12]\n",
      "    tensor[shape=torch.Size([1, 12, 1, 62]), sum=12.000000953674316]\n",
      "    tensor[shape=torch.Size([1, 12, 1, 62]), sum=12.0]\n",
      "    tensor[shape=torch.Size([1, 12, 1, 62]), sum=11.999999046325684]\n",
      "layer_idx 0\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=0.9999999403953552]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.9806371331214905]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_idx 1\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0000001192092896]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.8895342350006104]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_idx 2\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.875938892364502]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_idx 3\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=0.9999998807907104]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.6076318621635437]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_idx 4\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.6114931106567383]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.000000238418579]\n",
      "layer_idx 5\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.44242462515830994]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=0.9999998807907104]\n",
      "layer_idx 6\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.4186858534812927]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0000001192092896]\n",
      "layer_idx 7\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.42231568694114685]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_idx 8\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=0.9999999403953552]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.5211020708084106]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_idx 9\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.6318533420562744]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_idx 10\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0000001192092896]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.7322278618812561]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_idx 11\n",
      "layer_avg_attn: tensor[shape=torch.Size([1, 61]), sum=1.0]\n",
      "layer_avg_attn_cleaned: tensor[shape=torch.Size([62]), sum=0.8336085081100464]\n",
      "layer_avg_attn_normalized: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "layer_avg_attns: tensor[shape=torch.Size([1, 12, 1, 61]), sum=12.0]\n",
      "token_attn: tensor[shape=torch.Size([62]), sum=1.0]\n",
      "tensor([0.0000, 0.0243, 0.0139, 0.0168, 0.0155, 0.0147, 0.0050, 0.0046, 0.0035,\n",
      "        0.0034, 0.0025, 0.0024, 0.0019, 0.0019, 0.0022, 0.0038, 0.0024, 0.0027,\n",
      "        0.0032, 0.0035, 0.0071, 0.0055, 0.0062, 0.0032, 0.0041, 0.0042, 0.0197,\n",
      "        0.0076, 0.0089, 0.0070, 0.0057, 0.0035, 0.0035, 0.0105, 0.0173, 0.0160,\n",
      "        0.0280, 0.0162, 0.0057, 0.0058, 0.0055, 0.0052, 0.0063, 0.0054, 0.0053,\n",
      "        0.0053, 0.0091, 0.0122, 0.0101, 0.0146, 0.0175, 0.0217, 0.0238, 0.0236,\n",
      "        0.0184, 0.1339, 0.0803, 0.0386, 0.0369, 0.1077, 0.1046, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "def average_token_attn(token_attn):\n",
    "    layer_avg_attns = []\n",
    "    for layer_idx, layer_attn in enumerate(token_attn):\n",
    "        print('layer_idx', layer_idx)\n",
    "        # Take mean across model's attention heads\n",
    "        layer_avg_attn = layer_attn.squeeze(0).mean(dim=0)\n",
    "        inspect_shape('layer_avg_attn', layer_avg_attn)\n",
    "        layer_avg_attn_cleaned = torch.concat([\n",
    "            torch.tensor([0.]),  # First entry is null attention, set it to 0\n",
    "            # Remove all tokens except the most recent (since the first token generated has entire prompts' worth of attention generated) and remove the first entry\n",
    "            layer_avg_attn[-1][1:],\n",
    "            torch.tensor([0.]),  # Add a 0 for the current token itself\n",
    "        ])\n",
    "        inspect_shape('layer_avg_attn_cleaned', layer_avg_attn_cleaned)\n",
    "        layer_avg_attn_normalized = layer_avg_attn_cleaned / layer_avg_attn_cleaned.sum()\n",
    "        inspect_shape('layer_avg_attn_normalized', layer_avg_attn_normalized)\n",
    "        layer_avg_attns.append(layer_avg_attn_normalized)\n",
    "    inspect_shape('layer_avg_attns', layer_attn)\n",
    "    return torch.stack(layer_avg_attns).mean(dim=0)\n",
    "\n",
    "\n",
    "def generate_and_attn(prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    inspect_shape('tokens', tokens)\n",
    "\n",
    "    outputs = model.generate(tokens, max_new_tokens=48,\n",
    "                             output_attentions=True, return_dict_in_generate=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    inspect_shape('outputs.attentions', outputs.attentions)\n",
    "    token_attn = average_token_attn(outputs.attentions[1])\n",
    "    inspect_shape('token_attn', token_attn)\n",
    "    print(token_attn)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of \"one world, one dream\". The torch relay took place over 45km with 18 total runners.\n",
    "\n",
    "Q: What was the theme?\n",
    "A:\n",
    "\"\"\".strip()\n",
    "\n",
    "generate_and_attn(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1826, 0.3651, 0.5477, 0.7303])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2, 3, 4])\n",
    "torch.nn.functional.normalize(a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
